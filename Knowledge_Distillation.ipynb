{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNIWBkaZkboiMx2ljbObEm0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonsuarez24/Notebooks/blob/main/Knowledge_Distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL65TKqxWLfJ",
        "outputId": "cfd08bdf-c39c-4597-9006-88c967453ce0"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P3vIEfqWDbF",
        "outputId": "fb37958f-fa51-4285-94d5-8e0606471f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-178-15c2c979f39a>:13: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  from scipy.ndimage.interpolation import rotate as scipyrotate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.7.0->torchmetrics) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from scipy.ndimage.interpolation import rotate as scipyrotate\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "from torch.autograd import Function\n",
        "import torchvision.utils as vutils\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "import torchvision.utils as vutils\n",
        "!pip install torchmetrics\n",
        "from torchmetrics import Accuracy\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "8mEOqPQlWRsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "LtD9pee3WRDr"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time():\n",
        "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))"
      ],
      "metadata": {
        "id": "HN2XhgnEbuno"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Networks"
      ],
      "metadata": {
        "id": "01btqAhhWTKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Teacher"
      ],
      "metadata": {
        "id": "_DOuOpwoWW7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Student model\n",
        "class TeacherNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.relu1 = nn.LeakyReLU(0.2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(8*8*64, 10)  # Assuming input size (28, 28, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dgJ65Pz5WWVY"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = TeacherNetwork()\n",
        "print(summary(t, input_size=(12, 1, 28, 28)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG-kBmFQ1HCx",
        "outputId": "caf5c765-00fc-4cbe-c59c-861782c8be2d"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "TeacherNetwork                           [12, 10]                  --\n",
            "├─Conv2d: 1-1                            [12, 32, 14, 14]          320\n",
            "├─LeakyReLU: 1-2                         [12, 32, 14, 14]          --\n",
            "├─MaxPool2d: 1-3                         [12, 32, 15, 15]          --\n",
            "├─Conv2d: 1-4                            [12, 64, 8, 8]            18,496\n",
            "├─Flatten: 1-5                           [12, 4096]                --\n",
            "├─Linear: 1-6                            [12, 10]                  40,970\n",
            "==========================================================================================\n",
            "Total params: 59,786\n",
            "Trainable params: 59,786\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 15.45\n",
            "==========================================================================================\n",
            "Input size (MB): 0.04\n",
            "Forward/backward pass size (MB): 1.00\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 1.27\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student"
      ],
      "metadata": {
        "id": "0KgbVA5xWYWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Student model\n",
        "class StudentNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=2, padding=1)\n",
        "        self.relu1 = nn.LeakyReLU(0.2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(8*8*8, 10)  # Assuming input size (28, 28, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JgOvVjFuWZim"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = StudentNetwork()\n",
        "print(summary(s, input_size=(12, 1, 28, 28)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeMHktxO29b7",
        "outputId": "744e8be5-3b18-4dcf-80b6-adfc79ac584c"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "StudentNetwork                           [12, 10]                  --\n",
            "├─Conv2d: 1-1                            [12, 4, 14, 14]           40\n",
            "├─LeakyReLU: 1-2                         [12, 4, 14, 14]           --\n",
            "├─MaxPool2d: 1-3                         [12, 4, 15, 15]           --\n",
            "├─Conv2d: 1-4                            [12, 8, 8, 8]             296\n",
            "├─Flatten: 1-5                           [12, 512]                 --\n",
            "├─Linear: 1-6                            [12, 10]                  5,130\n",
            "==========================================================================================\n",
            "Total params: 5,466\n",
            "Trainable params: 5,466\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 0.38\n",
            "==========================================================================================\n",
            "Input size (MB): 0.04\n",
            "Forward/backward pass size (MB): 0.13\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 0.18\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "eZnxcQdrah-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(dataset, data_path):\n",
        "\n",
        "    if dataset == 'MNIST':\n",
        "        channel = 1\n",
        "        im_size = (28, 28)\n",
        "        num_classes = 10\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = [str(c) for c in range(num_classes)]\n",
        "\n",
        "    elif dataset == 'FashionMNIST':\n",
        "        channel = 1\n",
        "        im_size = (28, 28)\n",
        "        num_classes = 10\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        dst_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "\n",
        "    else:\n",
        "        exit('unknown dataset: %s'%dataset)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(dst_test, batch_size=32, shuffle=False, num_workers=0)\n",
        "    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=32, shuffle=True, num_workers=0)\n",
        "    return channel, im_size, num_classes, class_names, dst_train, dst_test, testloader, trainloader"
      ],
      "metadata": {
        "id": "PoLDhxSzaijk"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configs"
      ],
      "metadata": {
        "id": "1p-I8b0Rbmus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = f'/content/drive/MyDrive/Proyectos/Knowledge Distillation/experiments/{get_time()}'\n",
        "\n",
        "tb_path_teacher = save_path + '/tensorboard_teacher'\n",
        "tb_path_student = save_path + '/tensorboard_student'\n",
        "tb_path_student_distilled = save_path + '/tensorboard_student_distilled'\n",
        "\n",
        "model_teacher = save_path + '/model_teacher'\n",
        "model_student = save_path + '/model_student'\n",
        "model_student_distilled = save_path + '/model_student_distilled'\n",
        "\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "os.makedirs(tb_path_teacher, exist_ok=True)\n",
        "os.makedirs(tb_path_student, exist_ok=True)\n",
        "os.makedirs(model_teacher, exist_ok=True)\n",
        "os.makedirs(model_student, exist_ok=True)\n",
        "os.makedirs(tb_path_student_distilled, exist_ok=True)\n",
        "os.makedirs(model_student_distilled, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "writer_teacher = SummaryWriter(tb_path_teacher)\n",
        "writer_student = SummaryWriter(tb_path_student)\n",
        "writer_student_distilled = SummaryWriter(tb_path_student_distilled)"
      ],
      "metadata": {
        "id": "vuzJhu4Fbnv_"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lr_net = 0.001\n",
        "teacher_epochs = 10\n",
        "student_epochs = 3\n",
        "dataset = 'FashionMNIST'\n",
        "data_path = 'data'\n",
        "channel, im_size, num_classes, class_names, dst_train, dst_test, testloader, trainloader = get_dataset(dataset, data_path)\n",
        "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes, top_k=1).to(device)"
      ],
      "metadata": {
        "id": "gjWxDO8tcV7q"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "utHZq_6QbYtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = TeacherNetwork().to(device)\n",
        "optimizer = torch.optim.Adam(teacher.parameters(), lr=lr_net)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(teacher_epochs):\n",
        "\n",
        "    train_loss = AverageMeter()\n",
        "    accuracy_train = AverageMeter()\n",
        "    data_loop_train = tqdm(enumerate(trainloader), total=len(trainloader), colour='red')\n",
        "\n",
        "    teacher.train(True)\n",
        "    for _, train_data in data_loop_train:\n",
        "        train_img, train_label = train_data\n",
        "        train_img = train_img.to(device)\n",
        "        train_label = train_label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_pred = teacher(train_img)\n",
        "        loss = criterion(train_pred, train_label)\n",
        "        acc = accuracy(train_pred, train_label)\n",
        "\n",
        "        train_loss.update(loss.item(), train_img.size(0))\n",
        "        accuracy_train.update(acc.item(), train_img.size(0))\n",
        "\n",
        "        dict_metrics = dict(loss = train_loss.avg, acc = accuracy_train.avg)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_loop_train.set_description(f'Train Epoch [{epoch + 1} / {teacher_epochs}]')\n",
        "        data_loop_train.set_postfix(**dict_metrics)\n",
        "\n",
        "        for key, value in dict_metrics.items():\n",
        "              writer_teacher.add_scalar(f'train_{key}', value, epoch)\n",
        "\n",
        "\n",
        "    # Evaluation phase\n",
        "    teacher.eval()\n",
        "    data_loop_test = tqdm(enumerate(testloader), total=len(testloader),colour='green')\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_loss = AverageMeter()\n",
        "        accuracy_test = AverageMeter()\n",
        "\n",
        "        for _, test_data in data_loop_test:\n",
        "            test_img, test_label = test_data\n",
        "            test_img = test_img.to(device)\n",
        "            test_label = test_label.to(device)\n",
        "\n",
        "            test_pred = teacher(test_img)\n",
        "            loss = criterion(test_pred, test_label)\n",
        "            acc = accuracy(test_pred, test_label)\n",
        "\n",
        "            test_loss.update(loss.item(), test_img.size(0))\n",
        "            accuracy_test.update(acc.item(), test_img.size(0))\n",
        "            dict_metrics = dict(loss = test_loss.avg, acc = accuracy_test.avg)\n",
        "\n",
        "            data_loop_test.set_description(f'Test  Epoch [{epoch + 1} / {teacher_epochs}]')\n",
        "            data_loop_test.set_postfix(**dict_metrics)\n",
        "\n",
        "            for key, value in dict_metrics.items():\n",
        "                writer_teacher.add_scalar(f'test_{key}', value, epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-23IwZZvbag-",
        "outputId": "2d953238-83a2-463b-d15b-92958d991c2b"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch [1 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:29<00:00, 62.54it/s, acc=0.849, loss=0.427]\n",
            "Test  Epoch [1 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 94.15it/s, acc=0.871, loss=0.352]\n",
            "Train Epoch [2 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:24<00:00, 77.55it/s, acc=0.887, loss=0.316]\n",
            "Test  Epoch [2 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 94.60it/s, acc=0.883, loss=0.329]\n",
            "Train Epoch [3 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:23<00:00, 79.63it/s, acc=0.898, loss=0.285]\n",
            "Test  Epoch [3 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 80.95it/s, acc=0.89, loss=0.308]\n",
            "Train Epoch [4 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:23<00:00, 78.18it/s, acc=0.905, loss=0.265]\n",
            "Test  Epoch [4 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 80.32it/s, acc=0.876, loss=0.33]\n",
            "Train Epoch [5 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:23<00:00, 79.46it/s, acc=0.91, loss=0.25]\n",
            "Test  Epoch [5 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 91.11it/s, acc=0.89, loss=0.307]\n",
            "Train Epoch [6 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:24<00:00, 77.70it/s, acc=0.913, loss=0.239]\n",
            "Test  Epoch [6 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 92.13it/s, acc=0.896, loss=0.294]\n",
            "Train Epoch [7 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:24<00:00, 77.97it/s, acc=0.919, loss=0.227]\n",
            "Test  Epoch [7 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 92.45it/s, acc=0.893, loss=0.311]\n",
            "Train Epoch [8 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:23<00:00, 78.63it/s, acc=0.922, loss=0.217]\n",
            "Test  Epoch [8 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 86.90it/s, acc=0.892, loss=0.311]\n",
            "Train Epoch [9 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:23<00:00, 80.41it/s, acc=0.924, loss=0.209]\n",
            "Test  Epoch [9 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 86.11it/s, acc=0.896, loss=0.307]\n",
            "Train Epoch [10 / 10]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:23<00:00, 79.20it/s, acc=0.927, loss=0.203]\n",
            "Test  Epoch [10 / 10]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 91.96it/s, acc=0.897, loss=0.31]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student = StudentNetwork().to(device)\n",
        "optimizer = torch.optim.Adam(student.parameters(), lr=lr_net)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(student_epochs):\n",
        "\n",
        "    train_loss = AverageMeter()\n",
        "    accuracy_train = AverageMeter()\n",
        "    data_loop_train = tqdm(enumerate(trainloader), total=len(trainloader), colour='red')\n",
        "\n",
        "    student.train(True)\n",
        "    for _, train_data in data_loop_train:\n",
        "        train_img, train_label = train_data\n",
        "        train_img = train_img.to(device)\n",
        "        train_label = train_label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_pred = student(train_img)\n",
        "        loss = criterion(train_pred, train_label)\n",
        "        acc = accuracy(train_pred, train_label)\n",
        "\n",
        "        train_loss.update(loss.item(), train_img.size(0))\n",
        "        accuracy_train.update(acc.item(), train_img.size(0))\n",
        "\n",
        "        dict_metrics = dict(loss = train_loss.avg, acc = accuracy_train.avg)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_loop_train.set_description(f'Train Epoch [{epoch + 1} / {student_epochs}]')\n",
        "        data_loop_train.set_postfix(**dict_metrics)\n",
        "\n",
        "        for key, value in dict_metrics.items():\n",
        "              writer_student.add_scalar(f'train_{key}', value, epoch)\n",
        "\n",
        "\n",
        "    # Evaluation phase\n",
        "    student.eval()\n",
        "    data_loop_test = tqdm(enumerate(testloader), total=len(testloader),colour='green')\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_loss = AverageMeter()\n",
        "        accuracy_test = AverageMeter()\n",
        "\n",
        "        for _, test_data in data_loop_test:\n",
        "            test_img, test_label = test_data\n",
        "            test_img = test_img.to(device)\n",
        "            test_label = test_label.to(device)\n",
        "\n",
        "            test_pred = student(test_img)\n",
        "            loss = criterion(test_pred, test_label)\n",
        "            acc = accuracy(test_pred, test_label)\n",
        "\n",
        "            test_loss.update(loss.item(), test_img.size(0))\n",
        "            accuracy_test.update(acc.item(), test_img.size(0))\n",
        "            dict_metrics = dict(loss = test_loss.avg, acc = accuracy_test.avg)\n",
        "\n",
        "            data_loop_test.set_description(f'Test  Epoch [{epoch + 1} / {student_epochs}]')\n",
        "            data_loop_test.set_postfix(**dict_metrics)\n",
        "\n",
        "            for key, value in dict_metrics.items():\n",
        "                writer_student.add_scalar(f'test_{key}', value, epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-M23-6cl89v",
        "outputId": "15004121-03aa-413d-bfa4-338d89897b35"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch [1 / 3]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:29<00:00, 63.31it/s, acc=0.797, loss=0.573]\n",
            "Test  Epoch [1 / 3]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 79.61it/s, acc=0.821, loss=0.486]\n",
            "Train Epoch [2 / 3]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:23<00:00, 81.18it/s, acc=0.852, loss=0.419]\n",
            "Test  Epoch [2 / 3]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 92.11it/s, acc=0.852, loss=0.42]\n",
            "Train Epoch [3 / 3]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:23<00:00, 78.59it/s, acc=0.863, loss=0.386]\n",
            "Test  Epoch [3 / 3]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 94.44it/s, acc=0.858, loss=0.407]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distillation"
      ],
      "metadata": {
        "id": "aWlpeq7VuCCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kl_div_loss = nn.KLDivLoss(log_target=True) # KL Divergence loss for soft targets\n",
        "loss_func = nn.CrossEntropyLoss()           # Cross entropy loss for true label loss\n",
        "temperature: float = 18\n",
        "alpha:float = 0.4\n",
        "teacher.eval()\n",
        "student_destilled = StudentNetwork().to(device)\n",
        "optimizer = torch.optim.Adam(student_destilled.parameters(), lr=lr_net)"
      ],
      "metadata": {
        "id": "EPPSdt2-atlx"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(student_epochs):\n",
        "\n",
        "    train_loss = AverageMeter()\n",
        "    accuracy_train = AverageMeter()\n",
        "    data_loop_train = tqdm(enumerate(trainloader), total=len(trainloader), colour='red')\n",
        "\n",
        "    student_destilled.train(True)\n",
        "\n",
        "    for _, train_data in data_loop_train:\n",
        "\n",
        "        train_img, train_label = train_data\n",
        "        train_img = train_img.to(device)\n",
        "        train_label = train_label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_pred = teacher(train_img)\n",
        "        student_pred = student_destilled(train_img)\n",
        "        student_loss = loss_func(student_pred, train_label)\n",
        "\n",
        "        soft_targets = F.log_softmax(teacher_pred / temperature, dim=-1)\n",
        "        soft_prob = F.log_softmax(student_pred / temperature, dim=-1)\n",
        "\n",
        "\n",
        "        distillation_loss = kl_div_loss(soft_prob, soft_targets)*temperature**2\n",
        "\n",
        "        loss = alpha * student_loss + (1-alpha) * distillation_loss\n",
        "\n",
        "        acc = accuracy(student_pred, train_label)\n",
        "\n",
        "        train_loss.update(loss.item(), train_img.size(0))\n",
        "        accuracy_train.update(acc.item(), train_img.size(0))\n",
        "\n",
        "        dict_metrics = dict(loss = train_loss.avg, acc = accuracy_train.avg)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_loop_train.set_description(f'Train Epoch [{epoch + 1} / {student_epochs}]')\n",
        "        data_loop_train.set_postfix(**dict_metrics)\n",
        "\n",
        "        for key, value in dict_metrics.items():\n",
        "              writer_student_distilled.add_scalar(f'train_{key}', value, epoch)\n",
        "\n",
        "            # Evaluation phase\n",
        "    student_destilled.eval()\n",
        "    data_loop_test = tqdm(enumerate(testloader), total=len(testloader),colour='green')\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_loss = AverageMeter()\n",
        "        accuracy_test = AverageMeter()\n",
        "\n",
        "        for _, test_data in data_loop_test:\n",
        "            test_img, test_label = test_data\n",
        "            test_img = test_img.to(device)\n",
        "            test_label = test_label.to(device)\n",
        "\n",
        "            teacher_pred = teacher(test_img)\n",
        "            student_pred = student_destilled(test_img)\n",
        "\n",
        "            soft_targets = F.log_softmax(teacher_pred / temperature, dim=-1)\n",
        "            soft_prob = F.log_softmax(student_pred / temperature, dim=-1)\n",
        "\n",
        "            student_loss = loss_func(student_pred, test_label)\n",
        "            distillation_loss = kl_div_loss(soft_prob, soft_targets)*temperature**2\n",
        "\n",
        "            loss = alpha * student_loss + (1-alpha) * distillation_loss\n",
        "\n",
        "            acc = accuracy(student_pred, test_label)\n",
        "\n",
        "            test_loss.update(loss.item(), test_img.size(0))\n",
        "            accuracy_test.update(acc.item(), test_img.size(0))\n",
        "            dict_metrics = dict(loss = test_loss.avg, acc = accuracy_test.avg)\n",
        "\n",
        "            data_loop_test.set_description(f'Test  Epoch [{epoch + 1} / {student_epochs}]')\n",
        "            data_loop_test.set_postfix(**dict_metrics)\n",
        "\n",
        "            for key, value in dict_metrics.items():\n",
        "                writer_student_distilled.add_scalar(f'test_{key}', value, epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed1aQQQeTaLa",
        "outputId": "d310cc9a-b555-43a8-a2e6-a16dddfb596c"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch [1 / 3]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:31<00:00, 59.80it/s, acc=0.793, loss=0.515]\n",
            "Test  Epoch [1 / 3]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 69.46it/s, acc=0.829, loss=0.364]\n",
            "Train Epoch [2 / 3]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:25<00:00, 72.42it/s, acc=0.853, loss=0.318]\n",
            "Test  Epoch [2 / 3]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 68.81it/s, acc=0.856, loss=0.303]\n",
            "Train Epoch [3 / 3]: 100%|\u001b[31m██████████\u001b[0m| 1875/1875 [00:25<00:00, 73.45it/s, acc=0.866, loss=0.28]\n",
            "Test  Epoch [3 / 3]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 69.72it/s, acc=0.865, loss=0.276]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USOu27olfrsH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}