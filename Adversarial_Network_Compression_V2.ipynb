{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonsuarez24/Notebooks/blob/main/Adversarial_Network_Compression_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL65TKqxWLfJ",
        "outputId": "09c3e2fe-e3d7-4a97-a78c-66edb186a84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P3vIEfqWDbF",
        "outputId": "a744ebbf-81ec-45da-e861-87f5fc922802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-15c2c979f39a>:13: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  from scipy.ndimage.interpolation import rotate as scipyrotate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.7.0->torchmetrics) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from scipy.ndimage.interpolation import rotate as scipyrotate\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "from torch.autograd import Function\n",
        "import torchvision.utils as vutils\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "import torchvision.utils as vutils\n",
        "!pip install torchmetrics\n",
        "from torchmetrics import Accuracy\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mEOqPQlWRsn"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LtD9pee3WRDr"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HN2XhgnEbuno"
      },
      "outputs": [],
      "source": [
        "def get_time():\n",
        "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01btqAhhWTKK"
      },
      "source": [
        "# Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DOuOpwoWW7Y"
      },
      "source": [
        "##Teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dgJ65Pz5WWVY"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.fc = nn.Linear(512,512)\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        features = self.fc(out)\n",
        "        logits = self.classifier(features)\n",
        "        return logits, features\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG-kBmFQ1HCx",
        "outputId": "84a073e5-2e6e-4b8d-96de-7426f4936547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "VGG                                      [12, 10]                  --\n",
            "├─Sequential: 1-1                        [12, 512, 1, 1]           --\n",
            "│    └─Conv2d: 2-1                       [12, 64, 32, 32]          1,792\n",
            "│    └─BatchNorm2d: 2-2                  [12, 64, 32, 32]          128\n",
            "│    └─ReLU: 2-3                         [12, 64, 32, 32]          --\n",
            "│    └─MaxPool2d: 2-4                    [12, 64, 16, 16]          --\n",
            "│    └─Conv2d: 2-5                       [12, 128, 16, 16]         73,856\n",
            "│    └─BatchNorm2d: 2-6                  [12, 128, 16, 16]         256\n",
            "│    └─ReLU: 2-7                         [12, 128, 16, 16]         --\n",
            "│    └─MaxPool2d: 2-8                    [12, 128, 8, 8]           --\n",
            "│    └─Conv2d: 2-9                       [12, 256, 8, 8]           295,168\n",
            "│    └─BatchNorm2d: 2-10                 [12, 256, 8, 8]           512\n",
            "│    └─ReLU: 2-11                        [12, 256, 8, 8]           --\n",
            "│    └─Conv2d: 2-12                      [12, 256, 8, 8]           590,080\n",
            "│    └─BatchNorm2d: 2-13                 [12, 256, 8, 8]           512\n",
            "│    └─ReLU: 2-14                        [12, 256, 8, 8]           --\n",
            "│    └─MaxPool2d: 2-15                   [12, 256, 4, 4]           --\n",
            "│    └─Conv2d: 2-16                      [12, 512, 4, 4]           1,180,160\n",
            "│    └─BatchNorm2d: 2-17                 [12, 512, 4, 4]           1,024\n",
            "│    └─ReLU: 2-18                        [12, 512, 4, 4]           --\n",
            "│    └─Conv2d: 2-19                      [12, 512, 4, 4]           2,359,808\n",
            "│    └─BatchNorm2d: 2-20                 [12, 512, 4, 4]           1,024\n",
            "│    └─ReLU: 2-21                        [12, 512, 4, 4]           --\n",
            "│    └─MaxPool2d: 2-22                   [12, 512, 2, 2]           --\n",
            "│    └─Conv2d: 2-23                      [12, 512, 2, 2]           2,359,808\n",
            "│    └─BatchNorm2d: 2-24                 [12, 512, 2, 2]           1,024\n",
            "│    └─ReLU: 2-25                        [12, 512, 2, 2]           --\n",
            "│    └─Conv2d: 2-26                      [12, 512, 2, 2]           2,359,808\n",
            "│    └─BatchNorm2d: 2-27                 [12, 512, 2, 2]           1,024\n",
            "│    └─ReLU: 2-28                        [12, 512, 2, 2]           --\n",
            "│    └─MaxPool2d: 2-29                   [12, 512, 1, 1]           --\n",
            "│    └─AvgPool2d: 2-30                   [12, 512, 1, 1]           --\n",
            "├─Linear: 1-2                            [12, 512]                 262,656\n",
            "├─Linear: 1-3                            [12, 10]                  5,130\n",
            "==========================================================================================\n",
            "Total params: 9,493,770\n",
            "Trainable params: 9,493,770\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 1.84\n",
            "==========================================================================================\n",
            "Input size (MB): 0.15\n",
            "Forward/backward pass size (MB): 29.15\n",
            "Params size (MB): 37.98\n",
            "Estimated Total Size (MB): 67.27\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "t = VGG('VGG11')\n",
        "print(summary(t, input_size=(12, 3, 32, 32)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KgbVA5xWYWR"
      },
      "source": [
        "## Student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JgOvVjFuWZim"
      },
      "outputs": [],
      "source": [
        "class StudentNetwork(nn.Module):\n",
        "    def __init__(self, channel, num_classes):\n",
        "        super(StudentNetwork, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(channel, 6, kernel_size=5, padding=2 if channel==1 else 0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc_1 = nn.Linear(16 * 5 * 5, 256)\n",
        "        self.fc_2 = nn.Linear(256, 512)\n",
        "        self.fc_3 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc_1(x))\n",
        "        features = F.relu(self.fc_2(x))\n",
        "        logits = self.fc_3(features)\n",
        "        return logits, features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeMHktxO29b7",
        "outputId": "bf79c9e8-0866-44cd-f447-648e0291391d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "StudentNetwork                           [12, 10]                  --\n",
            "├─Sequential: 1-1                        [12, 16, 5, 5]            --\n",
            "│    └─Conv2d: 2-1                       [12, 6, 28, 28]           456\n",
            "│    └─ReLU: 2-2                         [12, 6, 28, 28]           --\n",
            "│    └─MaxPool2d: 2-3                    [12, 6, 14, 14]           --\n",
            "│    └─Conv2d: 2-4                       [12, 16, 10, 10]          2,416\n",
            "│    └─ReLU: 2-5                         [12, 16, 10, 10]          --\n",
            "│    └─MaxPool2d: 2-6                    [12, 16, 5, 5]            --\n",
            "├─Linear: 1-2                            [12, 256]                 102,656\n",
            "├─Linear: 1-3                            [12, 512]                 131,584\n",
            "├─Linear: 1-4                            [12, 10]                  5,130\n",
            "==========================================================================================\n",
            "Total params: 242,242\n",
            "Trainable params: 242,242\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 10.06\n",
            "==========================================================================================\n",
            "Input size (MB): 0.15\n",
            "Forward/backward pass size (MB): 0.68\n",
            "Params size (MB): 0.97\n",
            "Estimated Total Size (MB): 1.80\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "s = StudentNetwork(3,10)\n",
        "print(summary(s, input_size=(12, 3, 32, 32)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxNlwvfeaE3H"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nDeAxQIraGOd"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.output_layer = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, drop):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        if drop == True:\n",
        "            x = self.dropout(x)\n",
        "        x = self.sigmoid(self.output_layer(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7YuH7ULwxPf",
        "outputId": "47e5c088-73bd-460e-af1f-95371294db25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 1])\n"
          ]
        }
      ],
      "source": [
        "model = Discriminator(100)\n",
        "input_data = torch.randn(12,100)  # Example input with shape [batch_size, channels, height, width]\n",
        "output = model(input_data, True)\n",
        "print(output.shape)  # Should be [12, 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZnxcQdrah-e"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PoLDhxSzaijk"
      },
      "outputs": [],
      "source": [
        "def get_dataset(dataset, data_path):\n",
        "\n",
        "    if dataset == 'MNIST':\n",
        "        channel = 1\n",
        "        im_size = (28, 28)\n",
        "        num_classes = 10\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = [str(c) for c in range(num_classes)]\n",
        "\n",
        "    elif dataset == 'FashionMNIST':\n",
        "        channel = 1\n",
        "        im_size = (28, 28)\n",
        "        num_classes = 10\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        dst_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "\n",
        "    elif dataset == 'CIFAR10':\n",
        "        channel = 3\n",
        "        im_size = (32, 32)\n",
        "        num_classes = 10\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        #transform = transforms.Compose([transforms.ToTensor(), transforms.RandomHorizontalFlip(0.5)])\n",
        "        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "\n",
        "    else:\n",
        "        exit('unknown dataset: %s'%dataset)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(dst_test, batch_size=32, shuffle=False, num_workers=0)\n",
        "    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=32, shuffle=True, num_workers=0)\n",
        "    return channel, im_size, num_classes, class_names, dst_train, dst_test, testloader, trainloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p-I8b0Rbmus"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vuzJhu4Fbnv_"
      },
      "outputs": [],
      "source": [
        "save_path = f'/content/drive/MyDrive/Proyectos/Adversarial Knowledge Distillation/experiments/{get_time()}'\n",
        "\n",
        "tb_path_teacher = save_path + '/tensorboard_teacher'\n",
        "tb_path_student = save_path + '/tensorboard_student'\n",
        "tb_path_student_distilled = save_path + '/tensorboard_student_distilled'\n",
        "\n",
        "model_teacher = save_path + '/model_teacher'\n",
        "model_student = save_path + '/model_student'\n",
        "model_student_distilled = save_path + '/model_student_distilled'\n",
        "\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "os.makedirs(tb_path_teacher, exist_ok=True)\n",
        "os.makedirs(tb_path_student, exist_ok=True)\n",
        "os.makedirs(model_teacher, exist_ok=True)\n",
        "os.makedirs(model_student, exist_ok=True)\n",
        "os.makedirs(tb_path_student_distilled, exist_ok=True)\n",
        "os.makedirs(model_student_distilled, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "writer_teacher = SummaryWriter(tb_path_teacher)\n",
        "writer_student = SummaryWriter(tb_path_student)\n",
        "writer_student_distilled = SummaryWriter(tb_path_student_distilled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWxDO8tcV7q",
        "outputId": "3febe9ac-03d3-48d2-ae08-b3313731287a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lr_net = 0.001\n",
        "teacher_epochs = 20\n",
        "student_epochs = 20\n",
        "dataset = 'CIFAR10'\n",
        "data_path = 'data'\n",
        "channel, im_size, num_classes, class_names, dst_train, dst_test, testloader, trainloader = get_dataset(dataset, data_path)\n",
        "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes, top_k=1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utHZq_6QbYtv"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = VGG('VGG16').to(device)\n",
        "optimizer = torch.optim.Adam(teacher.parameters(), lr=lr_net)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=6, gamma=0.5)\n",
        "\n",
        "for epoch in range(teacher_epochs):\n",
        "\n",
        "    train_loss = AverageMeter()\n",
        "    accuracy_train = AverageMeter()\n",
        "    data_loop_train = tqdm(enumerate(trainloader), total=len(trainloader), colour='red')\n",
        "\n",
        "    teacher.train(True)\n",
        "    for _, train_data in data_loop_train:\n",
        "        train_img, train_label = train_data\n",
        "        train_img = train_img.to(device)\n",
        "        train_label = train_label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_pred, _ = teacher(train_img)\n",
        "        loss = criterion(train_pred, train_label)\n",
        "        acc = accuracy(train_pred, train_label)\n",
        "\n",
        "        train_loss.update(loss.item(), train_img.size(0))\n",
        "        accuracy_train.update(acc.item(), train_img.size(0))\n",
        "\n",
        "        dict_metrics = dict(loss = train_loss.avg, acc = accuracy_train.avg)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_loop_train.set_description(f'Train Epoch [{epoch + 1} / {teacher_epochs}]')\n",
        "        data_loop_train.set_postfix(**dict_metrics)\n",
        "\n",
        "        for key, value in dict_metrics.items():\n",
        "              writer_teacher.add_scalar(f'train_{key}', value, epoch)\n",
        "\n",
        "    scheduler.step()\n",
        "    # Evaluation phase\n",
        "    teacher.eval()\n",
        "    data_loop_test = tqdm(enumerate(testloader), total=len(testloader),colour='green')\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_loss = AverageMeter()\n",
        "        accuracy_test = AverageMeter()\n",
        "\n",
        "        for _, test_data in data_loop_test:\n",
        "            test_img, test_label = test_data\n",
        "            test_img = test_img.to(device)\n",
        "            test_label = test_label.to(device)\n",
        "\n",
        "            test_pred, _ = teacher(test_img)\n",
        "            loss = criterion(test_pred, test_label)\n",
        "            acc = accuracy(test_pred, test_label)\n",
        "\n",
        "            test_loss.update(loss.item(), test_img.size(0))\n",
        "            accuracy_test.update(acc.item(), test_img.size(0))\n",
        "            dict_metrics = dict(loss = test_loss.avg, acc = accuracy_test.avg)\n",
        "\n",
        "            data_loop_test.set_description(f'Test  Epoch [{epoch + 1} / {teacher_epochs}]')\n",
        "            data_loop_test.set_postfix(**dict_metrics)\n",
        "\n",
        "            for key, value in dict_metrics.items():\n",
        "                writer_teacher.add_scalar(f'test_{key}', value, epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcTrQBvIUwFW",
        "outputId": "d4ba0afe-dbaa-478f-9ffa-265788dd4dc6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch [1 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:41<00:00, 15.44it/s, acc=0.258, loss=1.88]\n",
            "Test  Epoch [1 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:06<00:00, 51.39it/s, acc=0.352, loss=1.55]\n",
            "Train Epoch [2 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:30<00:00, 17.23it/s, acc=0.489, loss=1.36]\n",
            "Test  Epoch [2 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 56.97it/s, acc=0.582, loss=1.17]\n",
            "Train Epoch [3 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:37<00:00, 15.99it/s, acc=0.646, loss=1]\n",
            "Test  Epoch [3 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 52.39it/s, acc=0.646, loss=1.09]\n",
            "Train Epoch [4 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:33<00:00, 16.75it/s, acc=0.726, loss=0.802]\n",
            "Test  Epoch [4 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 57.45it/s, acc=0.747, loss=0.759]\n",
            "Train Epoch [5 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:33<00:00, 16.71it/s, acc=0.778, loss=0.664]\n",
            "Test  Epoch [5 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 56.28it/s, acc=0.79, loss=0.645]\n",
            "Train Epoch [6 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:33<00:00, 16.69it/s, acc=0.812, loss=0.564]\n",
            "Test  Epoch [6 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 56.34it/s, acc=0.809, loss=0.576]\n",
            "Train Epoch [7 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:33<00:00, 16.66it/s, acc=0.871, loss=0.389]\n",
            "Test  Epoch [7 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 55.14it/s, acc=0.84, loss=0.483]\n",
            "Train Epoch [8 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:35<00:00, 16.44it/s, acc=0.891, loss=0.327]\n",
            "Test  Epoch [8 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 54.60it/s, acc=0.846, loss=0.48]\n",
            "Train Epoch [9 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:32<00:00, 16.83it/s, acc=0.909, loss=0.272]\n",
            "Test  Epoch [9 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:06<00:00, 51.49it/s, acc=0.844, loss=0.499]\n",
            "Train Epoch [10 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:31<00:00, 17.04it/s, acc=0.925, loss=0.223]\n",
            "Test  Epoch [10 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:06<00:00, 49.51it/s, acc=0.839, loss=0.512]\n",
            "Train Epoch [11 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:32<00:00, 16.89it/s, acc=0.937, loss=0.188]\n",
            "Test  Epoch [11 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:06<00:00, 48.18it/s, acc=0.85, loss=0.505]\n",
            "Train Epoch [12 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:31<00:00, 17.03it/s, acc=0.948, loss=0.156]\n",
            "Test  Epoch [12 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:06<00:00, 51.48it/s, acc=0.842, loss=0.582]\n",
            "Train Epoch [13 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:33<00:00, 16.67it/s, acc=0.973, loss=0.0827]\n",
            "Test  Epoch [13 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 53.32it/s, acc=0.864, loss=0.517]\n",
            "Train Epoch [14 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:33<00:00, 16.73it/s, acc=0.98, loss=0.0616]\n",
            "Test  Epoch [14 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 56.13it/s, acc=0.851, loss=0.612]\n",
            "Train Epoch [15 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:33<00:00, 16.74it/s, acc=0.981, loss=0.0552]\n",
            "Test  Epoch [15 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 56.73it/s, acc=0.861, loss=0.603]\n",
            "Train Epoch [16 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:34<00:00, 16.51it/s, acc=0.986, loss=0.0437]\n",
            "Test  Epoch [16 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 53.44it/s, acc=0.863, loss=0.653]\n",
            "Train Epoch [17 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:31<00:00, 17.02it/s, acc=0.986, loss=0.0428]\n",
            "Test  Epoch [17 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:06<00:00, 48.29it/s, acc=0.857, loss=0.651]\n",
            "Train Epoch [18 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:32<00:00, 16.89it/s, acc=0.989, loss=0.0345]\n",
            "Test  Epoch [18 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:06<00:00, 48.51it/s, acc=0.861, loss=0.638]\n",
            "Train Epoch [19 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:30<00:00, 17.25it/s, acc=0.995, loss=0.0153]\n",
            "Test  Epoch [19 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 53.06it/s, acc=0.862, loss=0.764]\n",
            "Train Epoch [20 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [01:31<00:00, 17.14it/s, acc=0.996, loss=0.0122]\n",
            "Test  Epoch [20 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 57.44it/s, acc=0.865, loss=0.771]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-M23-6cl89v",
        "outputId": "236dd721-0df8-4f69-fdf6-d135277bd630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch [1 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:38<00:00, 40.08it/s, acc=0.366, loss=1.7]\n",
            "Test  Epoch [1 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 80.57it/s, acc=0.462, loss=1.48]\n",
            "Train Epoch [2 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.51it/s, acc=0.482, loss=1.42]\n",
            "Test  Epoch [2 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 80.12it/s, acc=0.492, loss=1.41]\n",
            "Train Epoch [3 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.63it/s, acc=0.528, loss=1.3]\n",
            "Test  Epoch [3 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 77.31it/s, acc=0.543, loss=1.27]\n",
            "Train Epoch [4 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:36<00:00, 42.25it/s, acc=0.563, loss=1.22]\n",
            "Test  Epoch [4 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 65.05it/s, acc=0.558, loss=1.24]\n",
            "Train Epoch [5 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.61it/s, acc=0.592, loss=1.15]\n",
            "Test  Epoch [5 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 73.16it/s, acc=0.564, loss=1.25]\n",
            "Train Epoch [6 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:38<00:00, 40.37it/s, acc=0.611, loss=1.09]\n",
            "Test  Epoch [6 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 79.40it/s, acc=0.566, loss=1.22]\n",
            "Train Epoch [7 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.69it/s, acc=0.655, loss=0.97]\n",
            "Test  Epoch [7 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 77.02it/s, acc=0.595, loss=1.17]\n",
            "Train Epoch [8 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:36<00:00, 42.37it/s, acc=0.671, loss=0.923]\n",
            "Test  Epoch [8 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 68.86it/s, acc=0.599, loss=1.16]\n",
            "Train Epoch [9 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.24it/s, acc=0.685, loss=0.886]\n",
            "Test  Epoch [9 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 65.00it/s, acc=0.599, loss=1.16]\n",
            "Train Epoch [10 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.56it/s, acc=0.699, loss=0.848]\n",
            "Test  Epoch [10 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 67.05it/s, acc=0.604, loss=1.16]\n",
            "Train Epoch [11 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.65it/s, acc=0.713, loss=0.812]\n",
            "Test  Epoch [11 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 77.47it/s, acc=0.597, loss=1.18]\n",
            "Train Epoch [12 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.26it/s, acc=0.725, loss=0.775]\n",
            "Test  Epoch [12 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 77.52it/s, acc=0.593, loss=1.23]\n",
            "Train Epoch [13 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:39<00:00, 39.90it/s, acc=0.756, loss=0.692]\n",
            "Test  Epoch [13 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 78.45it/s, acc=0.601, loss=1.21]\n",
            "Train Epoch [14 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:38<00:00, 40.96it/s, acc=0.766, loss=0.664]\n",
            "Test  Epoch [14 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 76.62it/s, acc=0.606, loss=1.24]\n",
            "Train Epoch [15 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:39<00:00, 39.14it/s, acc=0.774, loss=0.641]\n",
            "Test  Epoch [15 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 76.82it/s, acc=0.596, loss=1.28]\n",
            "Train Epoch [16 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:36<00:00, 42.64it/s, acc=0.785, loss=0.616]\n",
            "Test  Epoch [16 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 62.66it/s, acc=0.599, loss=1.3]\n",
            "Train Epoch [17 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:37<00:00, 41.96it/s, acc=0.793, loss=0.594]\n",
            "Test  Epoch [17 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 70.54it/s, acc=0.6, loss=1.32]\n",
            "Train Epoch [18 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:39<00:00, 39.66it/s, acc=0.801, loss=0.57]\n",
            "Test  Epoch [18 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 76.29it/s, acc=0.599, loss=1.34]\n",
            "Train Epoch [19 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:40<00:00, 39.05it/s, acc=0.826, loss=0.516]\n",
            "Test  Epoch [19 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 77.16it/s, acc=0.599, loss=1.36]\n",
            "Train Epoch [20 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [00:38<00:00, 40.80it/s, acc=0.832, loss=0.501]\n",
            "Test  Epoch [20 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 77.01it/s, acc=0.598, loss=1.4]\n"
          ]
        }
      ],
      "source": [
        "student = StudentNetwork(3,10).to(device)\n",
        "optimizer = torch.optim.Adam(student.parameters(), lr=lr_net)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=6, gamma=0.5)\n",
        "\n",
        "for epoch in range(student_epochs):\n",
        "\n",
        "    train_loss = AverageMeter()\n",
        "    accuracy_train = AverageMeter()\n",
        "    data_loop_train = tqdm(enumerate(trainloader), total=len(trainloader), colour='red')\n",
        "\n",
        "    student.train(True)\n",
        "    for _, train_data in data_loop_train:\n",
        "        train_img, train_label = train_data\n",
        "        train_img = train_img.to(device)\n",
        "        train_label = train_label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_pred, _ = student(train_img)\n",
        "        loss = criterion(train_pred, train_label)\n",
        "        acc = accuracy(train_pred, train_label)\n",
        "\n",
        "        train_loss.update(loss.item(), train_img.size(0))\n",
        "        accuracy_train.update(acc.item(), train_img.size(0))\n",
        "\n",
        "        dict_metrics = dict(loss = train_loss.avg, acc = accuracy_train.avg)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_loop_train.set_description(f'Train Epoch [{epoch + 1} / {student_epochs}]')\n",
        "        data_loop_train.set_postfix(**dict_metrics)\n",
        "\n",
        "        for key, value in dict_metrics.items():\n",
        "              writer_student.add_scalar(f'train_{key}', value, epoch)\n",
        "\n",
        "    scheduler.step()\n",
        "    # Evaluation phase\n",
        "    student.eval()\n",
        "    data_loop_test = tqdm(enumerate(testloader), total=len(testloader),colour='green')\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_loss = AverageMeter()\n",
        "        accuracy_test = AverageMeter()\n",
        "\n",
        "        for _, test_data in data_loop_test:\n",
        "            test_img, test_label = test_data\n",
        "            test_img = test_img.to(device)\n",
        "            test_label = test_label.to(device)\n",
        "\n",
        "            test_pred, _ = student(test_img)\n",
        "            loss = criterion(test_pred, test_label)\n",
        "            acc = accuracy(test_pred, test_label)\n",
        "\n",
        "            test_loss.update(loss.item(), test_img.size(0))\n",
        "            accuracy_test.update(acc.item(), test_img.size(0))\n",
        "            dict_metrics = dict(loss = test_loss.avg, acc = accuracy_test.avg)\n",
        "\n",
        "            data_loop_test.set_description(f'Test  Epoch [{epoch + 1} / {student_epochs}]')\n",
        "            data_loop_test.set_postfix(**dict_metrics)\n",
        "\n",
        "            for key, value in dict_metrics.items():\n",
        "                writer_student.add_scalar(f'test_{key}', value, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWlpeq7VuCCf"
      },
      "source": [
        "# Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "EPPSdt2-atlx"
      },
      "outputs": [],
      "source": [
        "real_label = 1\n",
        "fake_label = 0\n",
        "student_distilled = StudentNetwork(3,10).to(device)\n",
        "teacher.eval()\n",
        "discriminator = Discriminator(512).to(device)\n",
        "bce_loss = nn.BCELoss()\n",
        "mse_loss = nn.MSELoss()\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr_net)\n",
        "optimizer_student = torch.optim.Adam(student_distilled.parameters(), lr=lr_net)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "scheduler_discriminator = StepLR(optimizer_discriminator, step_size=6, gamma=0.5)\n",
        "scheduler_student = StepLR(optimizer_student, step_size=6, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Ed1aQQQeTaLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f19e98-b8cb-4ca7-9a67-019283ee22c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train  Epoch [1 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:15<00:00,  7.98it/s, disc_loss=1.91, stu_loss=60.4]\n",
            "Test  Epoch [1 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 80.61it/s, acc=0.452, loss=2.91]\n",
            "Train  Epoch [2 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:19<00:00,  7.85it/s, disc_loss=1.91, stu_loss=46.1]\n",
            "Test  Epoch [2 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:05<00:00, 58.07it/s, acc=0.501, loss=2.69]\n",
            "Train  Epoch [3 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:16<00:00,  7.95it/s, disc_loss=1.91, stu_loss=41]\n",
            "Test  Epoch [3 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 71.76it/s, acc=0.51, loss=3.26]\n",
            "Train  Epoch [4 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:14<00:00,  8.05it/s, disc_loss=1.91, stu_loss=37.9]\n",
            "Test  Epoch [4 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 74.81it/s, acc=0.552, loss=2.76]\n",
            "Train  Epoch [5 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:14<00:00,  8.04it/s, disc_loss=1.91, stu_loss=35.1]\n",
            "Test  Epoch [5 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 77.62it/s, acc=0.576, loss=2.51]\n",
            "Train  Epoch [6 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:12<00:00,  8.14it/s, disc_loss=1.91, stu_loss=32.9]\n",
            "Test  Epoch [6 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 78.99it/s, acc=0.584, loss=2.37]\n",
            "Train  Epoch [7 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:15<00:00,  7.99it/s, disc_loss=1.91, stu_loss=29.3]\n",
            "Test  Epoch [7 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 69.19it/s, acc=0.618, loss=2.38]\n",
            "Train  Epoch [8 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:18<00:00,  7.88it/s, disc_loss=1.91, stu_loss=28.1]\n",
            "Test  Epoch [8 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 74.78it/s, acc=0.615, loss=2.44]\n",
            "Train  Epoch [9 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:15<00:00,  7.98it/s, disc_loss=1.91, stu_loss=27.1]\n",
            "Test  Epoch [9 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 65.89it/s, acc=0.623, loss=2.22]\n",
            "Train  Epoch [10 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:13<00:00,  8.08it/s, disc_loss=1.91, stu_loss=26.2]\n",
            "Test  Epoch [10 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 70.95it/s, acc=0.622, loss=2.39]\n",
            "Train  Epoch [11 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:13<00:00,  8.06it/s, disc_loss=1.91, stu_loss=25.3]\n",
            "Test  Epoch [11 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 69.97it/s, acc=0.62, loss=2.38]\n",
            "Train  Epoch [12 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:14<00:00,  8.05it/s, disc_loss=1.91, stu_loss=24.6]\n",
            "Test  Epoch [12 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 74.79it/s, acc=0.634, loss=2.29]\n",
            "Train  Epoch [13 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:14<00:00,  8.06it/s, disc_loss=1.91, stu_loss=22.8]\n",
            "Test  Epoch [13 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 75.17it/s, acc=0.637, loss=2.29]\n",
            "Train  Epoch [14 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:15<00:00,  7.98it/s, disc_loss=1.91, stu_loss=22.3]\n",
            "Test  Epoch [14 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 68.80it/s, acc=0.635, loss=2.3]\n",
            "Train  Epoch [15 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:17<00:00,  7.92it/s, disc_loss=1.91, stu_loss=21.9]\n",
            "Test  Epoch [15 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 63.45it/s, acc=0.641, loss=2.24]\n",
            "Train  Epoch [16 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:15<00:00,  8.00it/s, disc_loss=1.91, stu_loss=21.6]\n",
            "Test  Epoch [16 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 69.99it/s, acc=0.644, loss=2.29]\n",
            "Train  Epoch [17 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:12<00:00,  8.13it/s, disc_loss=1.91, stu_loss=21.3]\n",
            "Test  Epoch [17 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 65.28it/s, acc=0.644, loss=2.26]\n",
            "Train  Epoch [18 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:12<00:00,  8.13it/s, disc_loss=1.91, stu_loss=20.9]\n",
            "Test  Epoch [18 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 70.33it/s, acc=0.647, loss=2.31]\n",
            "Train  Epoch [19 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:12<00:00,  8.11it/s, disc_loss=1.91, stu_loss=20]\n",
            "Test  Epoch [19 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:04<00:00, 72.19it/s, acc=0.647, loss=2.33]\n",
            "Train  Epoch [20 / 20]: 100%|\u001b[31m██████████\u001b[0m| 1563/1563 [03:16<00:00,  7.95it/s, disc_loss=1.91, stu_loss=19.8]\n",
            "Test  Epoch [20 / 20]: 100%|\u001b[32m██████████\u001b[0m| 313/313 [00:03<00:00, 78.60it/s, acc=0.648, loss=2.34]\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(student_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    data_loop_train = tqdm(enumerate(trainloader), total=len(trainloader), colour='red')\n",
        "    student_distilled.train(True)\n",
        "    train_disc_loss = AverageMeter()\n",
        "    train_stu_loss = AverageMeter()\n",
        "    for _, train_data in data_loop_train:\n",
        "\n",
        "        # UPDATE DISCRIMINATOR ADVERSARIAL\n",
        "\n",
        "        train_img, _ = train_data\n",
        "        train_img = train_img.to(device)\n",
        "        batch_size = train_img.size(0)\n",
        "        optimizer_discriminator.zero_grad()\n",
        "\n",
        "        disc_label_real = torch.full((batch_size,), real_label, dtype=torch.float).unsqueeze(-1).to(device)\n",
        "        disc_label_fake = torch.full((batch_size,), fake_label, dtype=torch.float).unsqueeze(-1).to(device)\n",
        "\n",
        "        _, features_teacher = teacher(train_img)\n",
        "        output_real = discriminator(features_teacher, False)\n",
        "        err_real = bce_loss(output_real, disc_label_real)\n",
        "\n",
        "        _, features_student = student_distilled(train_img)\n",
        "        output_fake = discriminator(features_student, False)\n",
        "        err_real = bce_loss(output_fake, disc_label_fake)\n",
        "\n",
        "        # REGULARIZER DISCRIMINATOR\n",
        "        reg = bce_loss(output_fake, disc_label_real)\n",
        "        loss = err_real + err_real + reg\n",
        "        loss.backward()\n",
        "        optimizer_discriminator.step()\n",
        "\n",
        "        # UPDATE STUDENT ADVERSARIAL + MSE\n",
        "        optimizer_student.zero_grad()\n",
        "        stu_labels = torch.full((batch_size,), real_label, dtype=torch.float).unsqueeze(-1).to(device)\n",
        "        logits_student, features_student = student_distilled(train_img)\n",
        "        logits_teacher, features_teacher = teacher(train_img)\n",
        "        output_fake = discriminator(features_student, True)\n",
        "        s_loss = bce_loss(output_fake, stu_labels) + mse_loss(logits_student, logits_teacher)\n",
        "\n",
        "        s_loss.backward()\n",
        "        optimizer_student.step()\n",
        "\n",
        "        # METRICS\n",
        "        train_disc_loss.update(loss.item(), train_img.size(0))\n",
        "        train_stu_loss.update(s_loss.item(), train_img.size(0))\n",
        "        dict_metrics = dict(disc_loss = train_disc_loss.avg, stu_loss = train_stu_loss.avg)\n",
        "\n",
        "        data_loop_train.set_description(f'Train  Epoch [{epoch + 1} / {student_epochs}]')\n",
        "        data_loop_train.set_postfix(**dict_metrics)\n",
        "\n",
        "        for key, value in dict_metrics.items():\n",
        "            writer_student_distilled.add_scalar(f'ttrain_{key}', value, epoch)\n",
        "\n",
        "    scheduler_discriminator.step()\n",
        "    scheduler_student.step()\n",
        "    # Eval model\n",
        "    student_distilled.eval()\n",
        "    data_loop_test = tqdm(enumerate(testloader), total=len(testloader),colour='green')\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_loss = AverageMeter()\n",
        "        accuracy_test = AverageMeter()\n",
        "\n",
        "        for _, test_data in data_loop_test:\n",
        "            test_img, test_label = test_data\n",
        "            test_img = test_img.to(device)\n",
        "            test_label = test_label.to(device)\n",
        "\n",
        "            test_pred, _ = student_distilled(test_img)\n",
        "            loss = criterion(test_pred, test_label)\n",
        "            acc = accuracy(test_pred, test_label)\n",
        "\n",
        "            test_loss.update(loss.item(), test_img.size(0))\n",
        "            accuracy_test.update(acc.item(), test_img.size(0))\n",
        "            dict_metrics = dict(loss = test_loss.avg, acc = accuracy_test.avg)\n",
        "\n",
        "            data_loop_test.set_description(f'Test  Epoch [{epoch + 1} / {student_epochs}]')\n",
        "            data_loop_test.set_postfix(**dict_metrics)\n",
        "\n",
        "            for key, value in dict_metrics.items():\n",
        "                writer_student_distilled.add_scalar(f'test_{key}', value, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "USOu27olfrsH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}